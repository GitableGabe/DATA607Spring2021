---
title: "DATA607 Project 2"
author: "Gabriel Campos"
date: "`r format(Sys.Date(), '%B %d %Y')`"
output:
  html_document:
    code_folding: "hide"
    includes:
      in_header: header.html
    css: ./lab.css
    highlight: pygments
    theme: readable
    toc: true
    toc_float: true
  prettydoc::html_pretty:
    theme: cayman
  pdf_document: default
editor_options: 
  chunk_output_type: console
---


<!--match.arg(theme, themes()) : 
    'arg' should be one of "default", "cerulean", "journal","flatly",
    "darkly", "readable", "spacelab", "united", "cosmo", "lumen", "paper", 
    "sandstone", "simplex", "yeti"  -------->

```{r, echo=FALSE,warning=FALSE, results='hide', include=FALSE}
library(tidyverse)
library(dplyr)
library(reactable)
library(kableExtra)
library(stringr)
library(magrittr)
```

<!-- (https://rdrr.io/cran/reactable/man/reactable.html) -->


# Assignment Requirements{.tabset}

It can be useful to be able to classify new "test" documents using already classified "training"
documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to 
predict whether or not a new document is spam.  

For this project, you can start with a spam/ham dataset, then predict the class of new documents
(either withheld from the training dataset or from another source such as your own spam folder).
[Example corpus.](https://spamassassin.apache.org/old/publiccorpus/)

Here are two short videos that you may find helpful.

The first video shows how to unzip the provided files.

<iframe width="560" height="315" src="https://www.youtube.com/embed/6IzhRaSePKU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

The second video provides a short overview of predictive classifiers.

<iframe width="560" height="315" src="https://www.youtube.com/embed/5ikDo4SrLNQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

For more adventurous students, you are welcome (encouraged!) to come up with a different set of documents (including scraped web pages!?) that have already been classified (e.g. tagged), then analyze these documents to predict how new documents should be classified.

**New!  Project 4 extra credit!**  Students who use the relatively new tidymodels and textrecipes packages to complete their Project 4 work will automatically receive 5 extra credit points.  tidymodels is a significant improvement over Max Kuhn's older `caret` package.  Here are some resources to help you get up to speed on tidymodels and textrecipes.

* [Tidy Modeling with R, Max Kuhn and Julia Silge](https://www.tmwr.org/). Julia Silge has also done a number of tidymodels screencasts, including [here](https://www.youtube.com/watch?v=BgWCuyrwD1s)
* [github.com/tidymodels/textrecipes](https://github.com/tidymodels/textrecipes)
* DataCamp course, [Modeling with TidyModels in R](https://learn.datacamp.com/courses/modeling-with-tidymodels-in-r)